{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-OCR-2*\n",
        "!rm -rf ~/.cache/huggingface/modules/transformers_modules/deepseek-ai--DeepSeek-OCR-2*\n",
        "\n",
        "!pip uninstall -y transformers tokenizers flash-attn\n",
        "!pip uninstall -y torchvision torchaudio\n",
        "!pip install --no-cache torch==2.6.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install --no-cache transformers==4.46.3 tokenizers==0.20.3\n",
        "!pip install torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install einops addict easydict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)       # Should be 2.6.0\n",
        "print(torchvision.__version__) # Should be 0.21.0\n",
        "import transformers\n",
        "print(transformers.__version__) # Should be 4.46.3 from earlier fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install num2words python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import torch\n",
        "from num2words import num2words\n",
        "from docx import Document\n",
        "from docx.shared import Pt, Cm\n",
        "from docx.enum.table import WD_TABLE_ALIGNMENT\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from openai import OpenAI\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import time\n",
        "import random\n",
        "import gc\n",
        "import re\n",
        "\n",
        "# ================= –ù–ê–°–¢–†–û–ô–ö–ò =================\n",
        "API_KEY = \"your-openrouter-api-key-here\" \n",
        "BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "model_name = 'deepseek-ai/DeepSeek-OCR-2'\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "BATCH_FOLDERS = [\n",
        "    \"folder/path/one\", \n",
        "    \"folder/path/two\",\n",
        "    \"folder/path/three\"\n",
        "]\n",
        "\n",
        "OUTPUT_FILE = \"GOTOVAYA_OPIS_FULL.docx\"\n",
        "TEMP_OUTPUT_DIR = \"temp_ocr_results\"\n",
        "PROGRESS_FILE = \"progress.json\"  # –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "\n",
        "# ================= –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô =================\n",
        "logging.info(\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ DeepSeek-OCR...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\n",
        "    model_name, \n",
        "    trust_remote_code=True, \n",
        "    use_safetensors=True\n",
        ")\n",
        "model = model.eval().cuda().to(torch.bfloat16)\n",
        "\n",
        "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_ocr_text(text):\n",
        "    \"\"\"–£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã —Ä–∞–∑–º–µ—Ç–∫–∏\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    text = re.sub(r'<\\|ref\\|>.*?<\\|/ref\\|>', '', text)\n",
        "    text = re.sub(r'<\\|det\\|>\\[\\[.*?\\]\\]<\\|/det\\|>', '', text)\n",
        "    text = re.sub(r'<\\|grounding\\|>', '', text)\n",
        "    text = re.sub(r'<\\|.*?\\|>', '', text)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def collect_all_cases(batch_folders):\n",
        "    \"\"\"–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –ø–∞–ø–∫–∏ —Å –¥–µ–ª–∞–º–∏\"\"\"\n",
        "    all_cases = []\n",
        "    for batch_name in batch_folders:\n",
        "        if not os.path.exists(batch_name):\n",
        "            logging.warning(f\"–ü–∞–ø–∫–∞ '{batch_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –ü—Ä–æ–ø—É—Å–∫–∞—é.\")\n",
        "            continue\n",
        "        items = sorted(os.listdir(batch_name))\n",
        "        for item in items:\n",
        "            full_path = os.path.join(batch_name, item)\n",
        "            if os.path.isdir(full_path) and not item.startswith('.'):\n",
        "                all_cases.append(full_path)\n",
        "    return all_cases\n",
        "\n",
        "\n",
        "def run_ocr_on_folder(folder_path):\n",
        "    \"\"\"–ß–∏—Ç–∞–µ—Ç –í–°–ï —Ñ–æ—Ç–æ –≤–Ω—É—Ç—Ä–∏ –ø–∞–ø–∫–∏ –¥–µ–ª–∞\"\"\"\n",
        "    full_text = \"\"\n",
        "    files = sorted(os.listdir(folder_path))\n",
        "    image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "    \n",
        "    if not image_files:\n",
        "        return None\n",
        "\n",
        "    logging.info(f\"--- –û–±—Ä–∞–±–æ—Ç–∫–∞: {folder_path} ({len(image_files)} —Ñ–æ—Ç–æ) ---\")\n",
        "    \n",
        "    if not os.path.exists(TEMP_OUTPUT_DIR):\n",
        "        os.makedirs(TEMP_OUTPUT_DIR)\n",
        "\n",
        "    result_file = os.path.join(TEMP_OUTPUT_DIR, \"result.mmd\")\n",
        "\n",
        "    for filename in image_files:\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        \n",
        "        if os.path.exists(result_file):\n",
        "            os.remove(result_file)\n",
        "        \n",
        "        try:\n",
        "            prompt = \"<image>\\n<|grounding|>Convert the document to markdown. \"\n",
        "            res = model.infer(tokenizer, prompt=prompt, image_file=image_path, \n",
        "                            output_path=TEMP_OUTPUT_DIR, base_size=1024, \n",
        "                            image_size=768, crop_mode=True, save_results=True)\n",
        "            \n",
        "            if os.path.exists(result_file):\n",
        "                with open(result_file, 'r', encoding='utf-8') as f:\n",
        "                    text_content = f.read()\n",
        "                \n",
        "                text_content = clean_ocr_text(text_content)\n",
        "                \n",
        "                if text_content:\n",
        "                    full_text += f\"\\n\\n=== –§–ê–ô–õ: {filename} ===\\n\"\n",
        "                    full_text += text_content\n",
        "                    full_text += f\"\\n=== –ö–û–ù–ï–¶ –§–ê–ô–õ–ê ===\\n\"\n",
        "                    logging.info(f\"‚úì {filename}: –ø–æ–ª—É—á–µ–Ω–æ {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "                else:\n",
        "                    logging.warning(f\"‚úó {filename}: –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏\")\n",
        "            else:\n",
        "                logging.warning(f\"‚úó {filename}: result.mmd –Ω–µ —Å–æ–∑–¥–∞–Ω\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logging.error(f\"–û—à–∏–±–∫–∞ OCR {filename}: {e}\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    return full_text\n",
        "\n",
        "\n",
        "def analyze_structure_with_api(ocr_text, max_retries=6):\n",
        "    \"\"\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\"\"\"\n",
        "    \n",
        "    ocr_text = clean_ocr_text(ocr_text)\n",
        "    \n",
        "    system_prompt = \"\"\"\n",
        "–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Ä—Ö–∏–≤–∞—Ä–∏—É—Å. –ù–∞ –≤—Ö–æ–¥–µ ‚Äî OCR —Ç–µ–∫—Å—Ç–∞ –ø–∞–ø–∫–∏/–¥–µ–ª–∞.\n",
        "\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: —Å–æ—Å—Ç–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å—å –¥–ª—è –æ–ø–∏—Å–∏. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON (–±–µ–∑ ```).\n",
        "\n",
        "–ü–†–ê–í–ò–õ–ê:\n",
        "1) –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Äî –ù–ï–õ–¨–ó–Ø –ø–∏—Å–∞—Ç—å, —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç.\n",
        "2) –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞–±–æ—Ä —Ä–∞–∑–Ω–æ—Ä–æ–¥–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ ‚Äî \"–ü–æ–¥–±–æ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (‚Ä¶–≤–∏–¥—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤‚Ä¶)\".\n",
        "\n",
        "–ü–û–õ–Ø JSON:\n",
        "- \"title\": –∞—Ä—Ö–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ + –≤ —Å–∫–æ–±–∫–∞—Ö –≤–∏–¥—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
        "- \"date\": –∫—Ä–∞–π–Ω–∏–µ –¥–∞—Ç—ã (—Ñ–æ—Ä–º–∞—Ç \"2016\" –∏–ª–∏ \"2016-2017\"). –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî \"\"\n",
        "- \"pages\": –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤ (—á–∏—Å–ª–æ). –ï—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ ‚Äî \"\"\n",
        "- \"storage\": —Å—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é \"5 –ª–µ—Ç\"\n",
        "- \"index\": –∏–Ω–¥–µ–∫—Å/–Ω–æ–º–µ—Ä –¥–µ–ª–∞. –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî \"\"\n",
        "\n",
        "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: –¢–û–õ–¨–ö–û JSON-–æ–±—ä–µ–∫—Ç, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π.\n",
        "\"\"\"\n",
        "\n",
        "    truncated_text = ocr_text[:120000]\n",
        "    logging.info(f\"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ API —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π {len(truncated_text)} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
        "    logging.info(f\"–ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤:\\n{truncated_text[:500]}\")\n",
        "\n",
        "    base_delay = 1.5\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"xiaomi/mimo-v2-flash\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": f\"–í–æ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\\n\\n{truncated_text}\"},\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "                max_tokens=2500\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            logging.info(f\"–û—Ç–≤–µ—Ç API: {content}\")\n",
        "            \n",
        "            content = content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "            return json.loads(content)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"[API ERROR] –ü–æ–ø—ã—Ç–∫–∞ {attempt}/{max_retries}: {e}\")\n",
        "\n",
        "            if attempt == max_retries:\n",
        "                return {\n",
        "                    \"title\": \"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞\",\n",
        "                    \"date\": \"\",\n",
        "                    \"pages\": \"\",\n",
        "                    \"storage\": \"5 –ª–µ—Ç\",\n",
        "                    \"index\": \"\"\n",
        "                }\n",
        "\n",
        "            delay = base_delay * (2 ** (attempt - 1))\n",
        "            jitter = random.uniform(0.8, 1.2)\n",
        "            time.sleep(delay * jitter)\n",
        "\n",
        "\n",
        "# ================= –°–û–•–†–ê–ù–ï–ù–ò–ï/–ó–ê–ì–†–£–ó–ö–ê –ü–†–û–ì–†–ï–°–°–ê =================\n",
        "\n",
        "def save_progress(table_rows, processed_paths):\n",
        "    \"\"\"–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ JSON —Ñ–∞–π–ª\"\"\"\n",
        "    data = {\n",
        "        'table_rows': table_rows,\n",
        "        'processed_paths': processed_paths\n",
        "    }\n",
        "    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {len(table_rows)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "\n",
        "def load_progress():\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑ JSON —Ñ–∞–π–ª–∞\"\"\"\n",
        "    if os.path.exists(PROGRESS_FILE):\n",
        "        try:\n",
        "            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "            logging.info(f\"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–≥—Ä–µ—Å—Å: {len(data.get('table_rows', []))} –∑–∞–ø–∏—Å–µ–π\")\n",
        "            return data.get('table_rows', []), set(data.get('processed_paths', []))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}\")\n",
        "    return [], set()\n",
        "\n",
        "\n",
        "# ================= –°–û–ó–î–ê–ù–ò–ï WORD –î–û–ö–£–ú–ï–ù–¢–ê =================\n",
        "\n",
        "def create_opis_document(table_rows, output_path):\n",
        "    \"\"\"–°–æ–∑–¥–∞—ë—Ç Word –¥–æ–∫—É–º–µ–Ω—Ç —Å –æ–ø–∏—Å—å—é –ë–ï–ó —à–∞–±–ª–æ–Ω–∞\"\"\"\n",
        "    \n",
        "    doc = Document()\n",
        "    \n",
        "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∞–ª—å–±–æ–º–Ω–∞—è A4)\n",
        "    section = doc.sections[0]\n",
        "    section.page_width = Cm(29.7)\n",
        "    section.page_height = Cm(21)\n",
        "    section.left_margin = Cm(2)\n",
        "    section.right_margin = Cm(1.5)\n",
        "    section.top_margin = Cm(1.5)\n",
        "    section.bottom_margin = Cm(1.5)\n",
        "    \n",
        "    # === –®–ê–ü–ö–ê ===\n",
        "    def add_text(text, size=14, bold=False, center=False):\n",
        "        para = doc.add_paragraph()\n",
        "        if center:\n",
        "            para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "        run = para.add_run(text)\n",
        "        run.font.size = Pt(size)\n",
        "        run.font.name = 'Times New Roman'\n",
        "        run.font.bold = bold\n",
        "        return para\n",
        "    \n",
        "    add_text(\"–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ\", center=True)\n",
        "    add_text(\"¬´–ü–†–ò–ú–ï–† –ü–†–ò–ú–ï–† –ü–†–ò–ú–ï–†\", center=True)\n",
        "    \n",
        "    doc.add_paragraph()\n",
        "    \n",
        "    # –ë–ª–æ–∫ –£–¢–í–ï–†–ñ–î–ê–Æ\n",
        "    approve = doc.add_paragraph()\n",
        "    approve.alignment = WD_ALIGN_PARAGRAPH.RIGHT\n",
        "    run_app = approve.add_run(\n",
        "        \"–£–¢–í–ï–†–ñ–î–ê–Æ\\n\"\n",
        "        \"–†–£–ö–û–í–û–î–ò–¢–ï–õ–¨ —Å–ª—É–∂–±—ã\\n\"\n",
        "        \"____________–ò.–ò. –ò–í–ê–ù–û–í\\n\"\n",
        "        \"¬´___¬ª_____________ 2026 –≥.\"\n",
        "    )\n",
        "    run_app.font.size = Pt(12)\n",
        "    run_app.font.name = 'Times New Roman'\n",
        "    \n",
        "    doc.add_paragraph()\n",
        "    \n",
        "    add_text(\"–û–ø–∏—Å—å ‚Ññ ____\", size=16, bold=True, center=True)\n",
        "    add_text(\"–∑–∞ 2010-2026 –≥–æ–¥—ã\", size=14, center=True)\n",
        "    \n",
        "    doc.add_paragraph()\n",
        "    \n",
        "    # === –¢–ê–ë–õ–ò–¶–ê ===\n",
        "    table = doc.add_table(rows=1, cols=7)\n",
        "    table.style = 'Table Grid'\n",
        "    table.alignment = WD_TABLE_ALIGNMENT.CENTER\n",
        "    \n",
        "    # –ó–∞–≥–æ–ª–æ–≤–∫–∏\n",
        "    headers = ['‚Ññ –ø/–ø', '–ò–Ω–¥–µ–∫—Å –¥–µ–ª–∞', '–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–µ–ª–∞', '–î–∞—Ç–∞ –¥–µ–ª–∞', \n",
        "               '–°—Ä–æ–∫ —Ö—Ä–∞–Ω–µ–Ω–∏—è', '–ö–æ–ª-–≤–æ –ª–∏—Å—Ç–æ–≤', '–ü—Ä–∏–º.']\n",
        "    \n",
        "    header_row = table.rows[0]\n",
        "    for i, header in enumerate(headers):\n",
        "        cell = header_row.cells[i]\n",
        "        para = cell.paragraphs[0]\n",
        "        para.clear()\n",
        "        run = para.add_run(header)\n",
        "        run.font.bold = True\n",
        "        run.font.size = Pt(10)\n",
        "        run.font.name = 'Times New Roman'\n",
        "        para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "    \n",
        "    # –ù—É–º–µ—Ä–∞—Ü–∏—è –∫–æ–ª–æ–Ω–æ–∫\n",
        "    num_row = table.add_row()\n",
        "    for i in range(7):\n",
        "        cell = num_row.cells[i]\n",
        "        para = cell.paragraphs[0]\n",
        "        para.clear()\n",
        "        run = para.add_run(str(i + 1))\n",
        "        run.font.size = Pt(9)\n",
        "        run.font.name = 'Times New Roman'\n",
        "        para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "    \n",
        "    # === –î–ê–ù–ù–´–ï ===\n",
        "    for row_data in table_rows:\n",
        "        row = table.add_row()\n",
        "        \n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "        values = [\n",
        "            (str(row_data.get('num', '')), True),\n",
        "            (str(row_data.get('index', '') or ''), True),\n",
        "            (str(row_data.get('title', '–ù–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ')), False),\n",
        "            (str(row_data.get('date', '') or ''), True),\n",
        "            (str(row_data.get('storage', '5 –ª–µ—Ç') or '5 –ª–µ—Ç'), True),\n",
        "            (str(row_data.get('pages', '') or ''), True),\n",
        "            (str(row_data.get('note', '') or ''), False)\n",
        "        ]\n",
        "        \n",
        "        for i, (value, center) in enumerate(values):\n",
        "            cell = row.cells[i]\n",
        "            para = cell.paragraphs[0]\n",
        "            para.clear()\n",
        "            run = para.add_run(value)\n",
        "            run.font.size = Pt(10)\n",
        "            run.font.name = 'Times New Roman'\n",
        "            if center:\n",
        "                para.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "    \n",
        "    doc.add_paragraph()\n",
        "    \n",
        "    # === –ò–¢–û–ì–û–í–ê–Ø –ó–ê–ü–ò–°–¨ ===\n",
        "    total = len(table_rows)\n",
        "    try:\n",
        "        total_words = num2words(total, lang='ru')\n",
        "    except:\n",
        "        total_words = str(total)\n",
        "    \n",
        "    summary = doc.add_paragraph()\n",
        "    run_sum = summary.add_run(\n",
        "        f\"–í –¥–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª –æ–ø–∏—Å–∏ –≤–Ω–µ—Å–µ–Ω–æ {total} ({total_words}) –¥–µ–ª \"\n",
        "        f\"—Å ‚Ññ 1 –ø–æ ‚Ññ {total}, –≤ —Ç–æ–º —á–∏—Å–ª–µ:\\n\"\n",
        "        f\"–ª–∏—Ç–µ—Ä–Ω—ã–µ –Ω–æ–º–µ—Ä–∞: ‚Äî\\n\"\n",
        "        f\"–ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –Ω–æ–º–µ—Ä–∞: ‚Äî\"\n",
        "    )\n",
        "    run_sum.font.size = Pt(12)\n",
        "    run_sum.font.name = 'Times New Roman'\n",
        "    \n",
        "    doc.add_paragraph()\n",
        "    doc.add_paragraph()\n",
        "    \n",
        "    # –ü–æ–¥–ø–∏—Å–∏\n",
        "    signatures = doc.add_paragraph()\n",
        "    run_sign = signatures.add_run(\n",
        "        \"–ì–ª–∞–≤–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç _________________ –ò.–û.–§–∞–º–∏–ª–∏—è\\n\\n\"\n",
        "        \"¬´_____¬ª___________ 2026 –≥.\\n\\n\\n\"\n",
        "        \"–ü–µ—Ä–µ–¥–∞–ª ______________________________________________ –µ–¥. —Ö—Ä.\\n\"\n",
        "        \"                    (—Ü–∏—Ñ—Ä–∞–º–∏ –∏ –ø—Ä–æ–ø–∏—Å—å—é)\\n\\n\\n\"\n",
        "        \"–ì–ª–∞–≤–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –û–û–î _________________ –ò.–û.–§–∞–º–∏–ª–∏—è\\n\\n\"\n",
        "        \"¬´_____¬ª___________ 2026 –≥.\\n\\n\\n\"\n",
        "        \"–ü—Ä–∏–Ω—è–ª ______________________________________________ –µ–¥. —Ö—Ä.\\n\"\n",
        "        \"                    (—Ü–∏—Ñ—Ä–∞–º–∏ –∏ –ø—Ä–æ–ø–∏—Å—å—é)\\n\\n\"\n",
        "        \"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞ –∞—Ä—Ö–∏–≤–∞ _________________ –ò.–û.–§–∞–º–∏–ª–∏—è\\n\\n\"\n",
        "        \"¬´_____¬ª___________ 2026 –≥.\"\n",
        "    )\n",
        "    run_sign.font.size = Pt(12)\n",
        "    run_sign.font.name = 'Times New Roman'\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
        "    doc.save(output_path)\n",
        "    logging.info(f\"‚úÖ Word –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}\")\n",
        "\n",
        "\n",
        "# ================= –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø =================\n",
        "\n",
        "def main():\n",
        "    logging.info(\"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫...\")\n",
        "    all_cases_paths = collect_all_cases(BATCH_FOLDERS)\n",
        "    \n",
        "    if not all_cases_paths:\n",
        "        print(\"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –ø–∞–ø–∫–∏ —Å –¥–µ–ª–∞–º–∏!\")\n",
        "        return\n",
        "\n",
        "    print(f\"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –¥–µ–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(all_cases_paths)}\")\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "    table_rows, processed_paths = load_progress()\n",
        "    \n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—ã–π –Ω–æ–º–µ—Ä\n",
        "    start_num = len(table_rows) + 1\n",
        "    \n",
        "    if processed_paths:\n",
        "        print(f\"üìÇ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –¥–µ–ª–∞ #{start_num} (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(processed_paths)})\")\n",
        "\n",
        "    for idx, folder_path in enumerate(all_cases_paths, 1):\n",
        "        \n",
        "        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ\n",
        "        if folder_path in processed_paths:\n",
        "            continue\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        \n",
        "        markdown_text = run_ocr_on_folder(folder_path)\n",
        "        \n",
        "        if not markdown_text or len(markdown_text.strip()) < 50:\n",
        "            logging.warning(f\"–ü—É—Å—Ç–∞—è –ø–∞–ø–∫–∞ –∏–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö: {folder_path}\")\n",
        "            processed_paths.add(folder_path)\n",
        "            continue\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º OCR –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\n",
        "        debug_file = os.path.join(TEMP_OUTPUT_DIR, f\"debug_{len(table_rows)+1}.txt\")\n",
        "        with open(debug_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(markdown_text)\n",
        "            \n",
        "        data = analyze_structure_with_api(markdown_text)\n",
        "        \n",
        "        row = {\n",
        "            'num': len(table_rows) + 1,\n",
        "            'index': data.get('index', ''),\n",
        "            'title': data.get('title', '–ù–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ'),\n",
        "            'date': data.get('date', ''),\n",
        "            'storage': data.get('storage', '5 –ª–µ—Ç'),\n",
        "            'pages': data.get('pages', ''),\n",
        "            'note': ''\n",
        "        }\n",
        "        table_rows.append(row)\n",
        "        processed_paths.add(folder_path)\n",
        "        \n",
        "        print(f\"--> [{len(table_rows)}/{len(all_cases_paths)}] –ì–æ—Ç–æ–≤–æ: {row['title'][:80]}...\")\n",
        "        \n",
        "        # ‚≠ê –ê–í–¢–û–°–û–•–†–ê–ù–ï–ù–ò–ï –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –¥–µ–ª–∞!\n",
        "        save_progress(table_rows, list(processed_paths))\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "    if table_rows:\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON\n",
        "        final_json = \"final_records.json\"\n",
        "        with open(final_json, 'w', encoding='utf-8') as f:\n",
        "            json.dump(table_rows, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üíæ JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {final_json}\")\n",
        "        \n",
        "        # –°–æ–∑–¥–∞—ë–º Word\n",
        "        try:\n",
        "            create_opis_document(table_rows, OUTPUT_FILE)\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"‚úÖ –£–°–ü–ï–•! –û–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞: {OUTPUT_FILE}\")\n",
        "            print(f\"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–µ–ª: {len(table_rows)}\")\n",
        "            print(f\"{'='*50}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Word: {e}\")\n",
        "            print(f\"‚ö†Ô∏è Word –Ω–µ —Å–æ–∑–¥–∞–Ω, –Ω–æ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {final_json}\")\n",
        "    else:\n",
        "        print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ò–ó JSON (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å Word) ===\n",
        "\n",
        "def recreate_word_from_json(json_path=\"final_records.json\", output_path=\"OPIS_RESTORED.docx\"):\n",
        "    \"\"\"–ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë—Ç Word –∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–≥–æ JSON\"\"\"\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        table_rows = json.load(f)\n",
        "    \n",
        "    print(f\"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(table_rows)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {json_path}\")\n",
        "    create_opis_document(table_rows, output_path)\n",
        "    print(f\"‚úÖ Word —Å–æ–∑–¥–∞–Ω: {output_path}\")\n",
        "\n",
        "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ:\n",
        "# recreate_word_from_json(\"progress.json\", \"OPIS_FROM_PROGRESS.docx\")"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
